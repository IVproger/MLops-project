{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import extract_data\n",
    "from src.utils import init_hydra\n",
    "\n",
    "\n",
    "cfg = init_hydra(\"main\")\n",
    "df, _ = extract_data(\"v7.0\", cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cancelled\n",
       "False    799398\n",
       "True      16266\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Cancelled\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelled = df[df[\"Cancelled\"]==True]\n",
    "on_time = df[df[\"Cancelled\"]==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelled.shape, on_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_persent = (cancelled.shape[0]*100/on_time.shape[0]) / 100\n",
    "print(representative_persent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_time = on_time.sample(frac=representative_persent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.concat([cancelled,on_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting the pie chart\n",
    "df[\"Cancelled\"].value_counts().plot(kind='pie', autopct='%1.1f%%')  # autopct displays the percentage value\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pull features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def pull_features(df: DataFrame, required: list[str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extract only the required features from the dataframe\n",
    "    \"\"\"\n",
    "    # Check that the required columns are there\n",
    "    for c in required:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(\n",
    "                f\"Dataframe lacks one or more of the required columns: {c}\"\n",
    "            )\n",
    "    pulled_df = df.copy()\n",
    "    columns_to_drop = set(df.columns) - set(required)\n",
    "\n",
    "    pulled_df.drop(list(columns_to_drop), axis=1, inplace=True)\n",
    "\n",
    "    return pulled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required: list[str] = cfg.required\n",
    "df = pull_features(df, required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Drop NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Cancelled\"].value_counts())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Drop NaNs\n",
    "df.dropna(axis=1, inplace=True)\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Cancelled\"].value_counts())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fix and transfrom cyclic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import FunctionTransformer\n",
    "\n",
    "\n",
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "def encode_cyclic_time_data(df: DataFrame, col: str, period: int) -> DataFrame:\n",
    "    # Check that the column exists\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"{col} is expected in the dataframe, but not found.\")\n",
    "\n",
    "    # Encode data\n",
    "    df[col + \"_sin\"] = sin_transformer(period).fit_transform(df[col])\n",
    "    df[col + \"_cos\"] = cos_transformer(period).fit_transform(df[col])\n",
    "\n",
    "    # df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def fix_hhmm(df: DataFrame, col: str) -> tuple[DataFrame, str, str]:\n",
    "    # Encoding hours and minutes\n",
    "    colHH = col + \"HH\"\n",
    "    colMM = col + \"MM\"\n",
    "    df[colHH] = df[col].apply(lambda hhmm: hhmm // 100)\n",
    "    df[colMM] = df[col].apply(lambda hhmm: hhmm % 100)\n",
    "\n",
    "    df.drop([col], axis=1, inplace=True)\n",
    "    return (df, colHH, colMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix hhmm columns\n",
    "for c in cfg[\"hhmm\"]:\n",
    "    df, colHH, colMM = fix_hhmm(df, c)\n",
    "    df = encode_cyclic_time_data(df, colHH, 24)\n",
    "    df = encode_cyclic_time_data(df, colMM, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform cyclic features\n",
    "for tf in cfg[\"time_features\"]:\n",
    "    df = encode_cyclic_time_data(df, tf[0], tf[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature crossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_cross(df: DataFrame, col1: str, col2: str):\n",
    "    mean = df.groupby(col1)[col2].mean()\n",
    "    df = df.merge(mean, on=col1, suffixes=(\"\", f\"_{col1}Mean\"))\n",
    "\n",
    "\n",
    "feature_cross(df, \"DayofMonth\", \"Distance\")\n",
    "feature_cross(df, \"Quarter\", \"Distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"DayofMonth_DistanceMean\" in df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.get_dummies(df, columns=df.columns[df.dtypes == 'object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_feature(df: DataFrame, col: str, num_buckets=1000):\n",
    "    # Hashing with buckets\n",
    "    df[col] = df[col].map(\n",
    "        lambda text: int(hashlib.md5(text.encode()).hexdigest(), 16) % num_buckets\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns[df.dtypes == \"object\"]:\n",
    "    df = hash_feature(df, c, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.shape, dff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([df, dff.drop(['Cancelled'], axis=1)], axis=1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a random sample, for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"Cancelled\"], axis=1)\n",
    "y = df[\"Cancelled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT REQUIRED FOR XGBOOST\n",
    "\n",
    "# from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "\n",
    "# sc = StandardScaler()\n",
    "\n",
    "# df = sc.fit_transform(X)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    stratify=y,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "ros = RandomOverSampler(\n",
    "    random_state=1,\n",
    ")\n",
    "rus = RandomUnderSampler(\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.bincount(y_train))\n",
    "\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaving only Top Gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_gs = [\n",
    "    \"Quarter\",\n",
    "    \"Marketing_Airline_Network\",\n",
    "    \"DayofMonth\",\n",
    "    \"Operated_or_Branded_Code_Share_Partners\",\n",
    "    \"Airline\",\n",
    "    \"Operating_Airline\",\n",
    "    \"OriginWac\",\n",
    "    \"DayofMonth_sin\",\n",
    "    \"DayOfWeek\",\n",
    "    \"DestWac\",\n",
    "    \"CRSDepTimeHH\",\n",
    "]\n",
    "\n",
    "df.drop(\n",
    "    list(set(df.columns) - set(top_gs)),\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1500,random_state=42,max_depth=1500)\n",
    "\n",
    "rf.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=1500,\n",
    "    subsample=0.999,\n",
    "    colsample_bytree=0.9999999,\n",
    "    tree_method=\"hist\",\n",
    "    \n",
    ")\n",
    "xgb.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "y_pred = rf.predict(X_test.values)\n",
    "print(\"Accuracy:\", accuracy_score(y_pred, y_test))\n",
    "print(\"F1 score: %.3f\" % f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"Recall: %.3f\" % recall_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"Precision: %.3f\" % precision_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"AUC Score: %.3f\" % roc_auc_score(y_test, y_pred, average=\"weighted\"))\n",
    "\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "sn.heatmap(conf_mat/np.sum(conf_mat), annot=True,  fmt='.2%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accessing feature importances\n",
    "feature_importances = xgb.feature_importances_\n",
    "\n",
    "# Pairing feature names with their importance scores\n",
    "features = X.columns\n",
    "importance_scores = list(zip(features, feature_importances))\n",
    "\n",
    "# Sorting the features by importance\n",
    "sorted_importance_scores = sorted(importance_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Plotting the feature importance graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(\n",
    "    range(len(sorted_importance_scores)),\n",
    "    [score[1] for score in sorted_importance_scores],\n",
    ")\n",
    "plt.xticks(\n",
    "    range(len(sorted_importance_scores)),\n",
    "    [score[0] for score in sorted_importance_scores],\n",
    "    rotation=90,\n",
    ")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_importance_scores = [s[0] for s in sorted_importance_scores if s[1] >= 0.02]\n",
    "sorted_importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import extract_data\n",
    "from src.utils import init_hydra\n",
    "\n",
    "\n",
    "cfg = init_hydra(\"main\")\n",
    "df, _ = extract_data(\"v2.0\", cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required: list[str] = cfg.required\n",
    "df = pull_features(df, required)\n",
    "\n",
    "df.dropna(axis=1, inplace=True)\n",
    "df.isna().sum().sum()\n",
    "\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import FunctionTransformer\n",
    "\n",
    "\n",
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "def encode_cyclic_time_data(df: DataFrame, col: str, period: int) -> DataFrame:\n",
    "    # Check that the column exists\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"{col} is expected in the dataframe, but not found.\")\n",
    "\n",
    "    # Encode data\n",
    "    df[col + \"_sin\"] = sin_transformer(period).fit_transform(df[col])\n",
    "    df[col + \"_cos\"] = cos_transformer(period).fit_transform(df[col])\n",
    "\n",
    "    # df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def fix_hhmm(df: DataFrame, col: str) -> tuple[DataFrame, str, str]:\n",
    "    # Encoding hours and minutes\n",
    "    colHH = col + \"HH\"\n",
    "    colMM = col + \"MM\"\n",
    "    df[colHH] = df[col].apply(lambda hhmm: hhmm // 100)\n",
    "    df[colMM] = df[col].apply(lambda hhmm: hhmm % 100)\n",
    "\n",
    "    df.drop([col], axis=1, inplace=True)\n",
    "    return (df, colHH, colMM)\n",
    "\n",
    "\n",
    "# Fix hhmm columns\n",
    "for c in cfg[\"hhmm\"]:\n",
    "    df, colHH, colMM = fix_hhmm(df, c)\n",
    "    df = encode_cyclic_time_data(df, colHH, 24)\n",
    "    df = encode_cyclic_time_data(df, colMM, 60)\n",
    "    \n",
    "    \n",
    "# Transform cyclic features\n",
    "for tf in cfg[\"time_features\"]:\n",
    "    df = encode_cyclic_time_data(df, tf[0], tf[1])\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for c in df.columns[df.dtypes == \"object\"]:\n",
    "    df = hash_feature(df, c, 10)\n",
    "    \n",
    "# top_gs = [\n",
    "#     \"Quarter\",\n",
    "#     \"Marketing_Airline_Network\",\n",
    "#     \"DayofMonth\",\n",
    "#     \"Operated_or_Branded_Code_Share_Partners\",\n",
    "#     \"Airline\",\n",
    "#     \"Operating_Airline\",\n",
    "#     \"OriginWac\",\n",
    "#     \"DayofMonth_sin\",\n",
    "#     \"DayOfWeek\",\n",
    "#     \"DestWac\",\n",
    "#     \"CRSDepTimeHH\",\n",
    "#     \"Cancelled\"\n",
    "# ]\n",
    "\n",
    "# df.drop(\n",
    "#     list(set(df.columns) - set(top_gs)),\n",
    "#     axis=1,\n",
    "#     inplace=True,\n",
    "# )\n",
    "    \n",
    "X = df.drop([\"Cancelled\"], axis=1)\n",
    "y = df[\"Cancelled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = rf.predict(X.values)\n",
    "print(\"Accuracy:\", accuracy_score(y_pred, y))\n",
    "print(\"F1 score: %.3f\" % f1_score(y, y_pred, average=\"weighted\"))\n",
    "print(\"Recall: %.3f\" % recall_score(y, y_pred, average=\"weighted\"))\n",
    "print(\"Precision: %.3f\" % precision_score(y, y_pred, average=\"weighted\"))\n",
    "print(\"AUC Score: %.3f\" % roc_auc_score(y, y_pred, average=\"weighted\"))\n",
    "\n",
    "print(\"Classification report:\\n\", classification_report(y, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_mat = confusion_matrix(y, y_pred)\n",
    "sn.heatmap(conf_mat/np.sum(conf_mat), annot=True,  fmt='.2%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
